{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook - Data Upload\n",
    "This Notebooks transforms and uploads the  data to SAP HANA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "-  SAP HANA Python Client API for Machine Learning Algorithms:\n",
    "   https://pypi.org/project/hana-ml/\n",
    "\n",
    "-  SAP HANA Automated Predictive Library (APL):  \n",
    "   https://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/hana_ml.algorithms.apl.html\n",
    "   \n",
    "-  SAP HANA Predictive Analysis Library (PAL):\n",
    "   https://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/hana_ml.algorithms.pal.html\n",
    "   \n",
    "-  Package Dependencies: \n",
    "   https://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/2.0.06/en-US/Installation.html\n",
    "   \n",
    "-  Examples: \n",
    "   https://github.com/SAP-samples/hana-ml-samples/tree/main/Python-API/pal/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAP HANA ML Library\n",
    "You will be using the 'SAP HANA Python Client API for Machine Learning Algorithm'. Begin by ensuring that you have the correct version available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>__Note__, in case you have a version prior to 2.13.22072200, run the following cell to upgrade the library.<BR>Running this cell, even if you have already that version installed, does <u>not</u> do any harm.  \n",
    "The current hana-ml version on pypi.org  is 2.14.22101400 and can also be used certainly.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hana-ml==2.14.22101400\n",
    "#!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hana_ml\n",
    "print(hana_ml.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hana_address = '<HOSTNAME>' \n",
    "hana_port = 443 # Adjust if needed / as advised\n",
    "hana_user = 'YOURUSERNAME' \n",
    "hana_password = 'YOURPASSWORD' \n",
    "hana_encrypt = 'true' # Adjust if needed / as advised\n",
    "\n",
    "import hana_ml.dataframe as dataframe\n",
    "\n",
    "# Instantiate connection object\n",
    "conn = dataframe.ConnectionContext(address = hana_address,\n",
    "                                   port = 443, \n",
    "                                   user = hana_user, \n",
    "                                   password = hana_password, \n",
    "                                   encrypt = hana_encrypt,\n",
    "                                   sslValidateCertificate = 'false' \n",
    "                                  )\n",
    "\n",
    "\n",
    "# Control connection\n",
    "print(conn.connection.isconnected())\n",
    "print(conn.hana_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CSV file into a Python object (Pandas DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_data = pd.read_csv(r'.\\Emp_Churn_Original.csv', sep = ',')\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before uploading the data to SAP HANA Cloud, carry out a few transformations. Turn the column headers into upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns = map(str.upper, df_data.columns)\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data to SAP HANA  \n",
    "...and upload the Pandas DataFrame into a table called after your Username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_employeechurn = dataframe.create_dataframe_from_pandas(connection_context = conn, \n",
    "                                                   pandas_df = df_data, \n",
    "                                                   table_name = 'EMPLOYEE_CHURN_ORG',\n",
    "                                                   force = True,\n",
    "                                                   replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have uploaded the data already, there are multiple ways to create the HANA dataframe\n",
    "# hdf_employeechurn = conn.table('EMPLOYEE_CHURN_ORG') \n",
    "# hdf_employeechurn = conn.sql('Select * from EMPLOYEE_CHURN_ORG') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the uploaded data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#control the variable types in SAP HANA\n",
    "hdf_employeechurn.dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_employeechurn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Top 10 rows\n",
    "display(hdf_employeechurn.head(10).collect())\n",
    "display(hdf_employeechurn.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display row count\n",
    "display(hdf_employeechurn.count())\n",
    "#display(hdf_employeechurn.distinct('EMPLOYEE_ID').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore column descriptive statistics using the describe method\n",
    "hdf_employeechurn.describe().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping some columns PREVCOUNTRYLAT PREVCOUNTRYLON\n",
    "hdf_employeechurn=hdf_employeechurn.drop('PREVCOUNTRYLAT').drop('PREVCOUNTRYLON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hdf_employeechurn.agg([('count', 'EMPLOYEE_ID', 'N')], group_by='FLIGHT_RISK').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What could be next?  \n",
    "- Looking at missing values\n",
    "- Splitting Train / Test data samples ...\n",
    "- Exploring [data yoga approaches](https://blogs.sap.com/2022/08/22/data-yoga-it-is-all-about-finding-the-right-balance/) \n",
    "- Exploring use of APL, PAL classification algorithms ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
